---
title: "Computing with R and Hadoop"
output: 
  html_document:
    number_sections: yes
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---



* * * 

# Learning goals

1. Learn what Hadoop is
2. Learn about current R/Hadoop integrations
3. Learn *when* to use R with Hadoop
4. Learn *how* to use R with Hadoop (by example)

# Brief introduction to Hadoop

Hadoop is currently a widely used open source distributed computing platform (read: "cloud computing software").  Here's how the project briefly describes itself:



> [Apache Software Foundation - What is Apache Hadoop?](http://hadoop.apache.org/#What+Is+Apache+Hadoop%3F) 
>   <br />
>   <br />
> The Apache Hadoop project develops open-source software for reliable, scalable, **distributed computing**. The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering **local computation and storage**...
>  <br />
>  <br />
> (emphasis added)  


## Key features

Two of Hadoop's main features are particularly relevant to this talk:

| Feature | Solves problem |
| :- | :- |
| Distributed storage | How do we easily store and access large datasets? |
| Distributed/batch computing | How do we quickly run analyses on large datasets? |


## Key ideas


**Hadoop is software that...** 

<!--- need to add pictures for each of these points -->

1. Links together servers to form a (storage + computing) cluster.

2. Creates a distributed file system called **hdfs** which splits large data files into smaller pieces that servers across the cluster store.

    <!--- answer q: how does distributed storage facilitate distributed computing -->
    
3. Uses the MapReduce programming model to implement distributed (i.e., parallel) computing.

    <!--- answer q: how does mapreduce work -->
    
    *Note:* MapReduce achieves parallelization by running separate copies of an analysis  on the smaller pieces of data files stored across the cluster.  Parallelization will not occur when a data file is ``too small'' to be split into smaller pieces (by default less than 64MB or 128MB).
    
4. Hadoop's natural computing strength is to apply a function to **extract** "data" from each record in a dataset, **group** the function's outputs, and **compute** summaries of the grouped outputs.

    *Examples:* 
    
    1. Given a dataset of tax records for many individuals, you could easily **extract** salaries, **group** them by city, and **compute** average salaries by city with Hadoop.
    
    2. Given a large collection of Twitter tweets, with Hadoop you could easily apply natural language processing algorithms to **extract** tweet topics, **group** by date, and **compute** the most popular topics over time.
    
5. Although this is getting better, Hadoop less naturally applies iterative algorithms to datasets since iterative algorithms don't as immediately fit into the **extract**, **group**, and **compute/summarize** pattern.

    *Examples:*
    
    1. Given the same collection of tweets, it would take more programming and computational effort to compute clusters of users who most often discuss similar topics.
    
    2. Given a large dataset of sales records, it would take more programming and computational effort  to use the entire dataset to fit a logistic regression model that could help predict which customers will buy different products based on demographic information.
    
    *Note:* Sometimes data storage tricks and approximate algorithms can be used to help achieve efficient parallelization for these and related types of problems.



# R/Hadoop integrations

R/Hadoop **integrations** let people use Hadoop to execute R code and also use R code to access data stored in Hadoop.  Cloud computing and statistics are both contemporary, related topics so practitioners naturally wanted to combine the two.

## Key integration projects

| Project | Sponsors/Maintainers | 
| :- | :- | 
| RHadoop | RevolutionAnalytics |
| RHIPE | Loosely, people associated with the Deptartment of Statistics at Purdue |




## When to consider integrating R and Hadoop

You should consider using an R/Hadoop integration when the computational requirements of your project align with the natural strengths of R and Hadoop. In particular, there are several factors you should especially consider:

| Factor | Mantra | Guideline | 
| :- | :- | :- |
| R's natural strength | Use R for statistical computing | Consider integrating when your project can be solved using code available in R, or when it is not easily solved in other languages | 
| Hadoop's natural strength | Use Hadoop for distributed storage & batch computing | Consider integrating when your problem requires lots of storage or when it could benefit from parallelization  | 
| Coding effort | Work smart, not hard | R and Hadoop are tools, not ``cure-all'' panaceas. Consider **not** integrating if it is easier to solve your problem with other tools |
| Processing time | Work smart, not hard (2) | Although some problems can benefit from parallelization, consider **not** integrating if the gains are negligible since this can help you reduce the complexity of your project



The following table applies these general ideas to common analytic scenarios:


| Scenario | Use R/Hadoop? | Why? | Example
| :- | :-: | :- | :- |
| Analyzing small data stored in Hadoop | Y | R can quickly download data analyze it locally | Want to analyze summary datasets derived from map reduce jobs done in Hadoop |
| Extracting complex features from large data stored in Hadoop | Y | R has more built-in and contributed functions that analyze data than many standard programming languages | R is a natural language to use to write an algorithm or classifier that extracts information about objects contained in images |
| Applying prediction and classification models to datasets | Y | R is better at modeling than many standard programming languages | Using a logistic regression model to generate predictions in a large dataset |
| Implementing an "iteration-based" machine learning algorithm | Maybe | 1) Other languages may be faster than R for your analysis, 2) Hadoop reads and writes a lot of data to disks, other "big data" tools, like Spark (and SparkR) are designed for speed in these scenarios by working in memory | Training a k-means classification algorithm on a large dataset |
| Simple pre-processing of large data stored in Hadoop | N | Standard programming languages are much faster than R at executing many basic text and image processing tasks | Pre-processing twitter tweets for use in a natural language processing project |




# Example integrations

## Warm up

### Goals

1. Test connections between R and Hadoop
2. Run a basic script 


### Story


### Code 


```
# example directly copied from: https://github.com/RevolutionAnalytics/rmr2/blob/master/docs/tutorial.md

  library(rmr2)

  Sys.setenv(HADOOP_STREAMING = '/usr/local/Cellar/hadoop/2.4.0/libexec/share/hadoop/tools/lib/hadoop-streaming-2.4.0.jar', 
             HADOOP_CMD = '/usr/local/bin/hadoop')

  groups = rbinom(32, n = 50, prob = 0.4)
  tapply(groups, groups, length)  

  groups.bak = groups
  

  groups = to.dfs(groups)
  res = from.dfs(
    mapreduce(
      input = groups, 
      map = function(., v) keyval(v, 1), 
      reduce = 
        function(k, vv) 
          keyval(k, length(vv))))

  
```




## Applying regression models, analyzing output


### Goals

Types of integration to demonstrate


| Scenario | How? |
| :- | :- |
| Applying prediction and classification models to datasets | We plan to use our sales records to build a logistic regression model that can help predict which customers in the consumer dataset are likely to buy our products |
| Analyzing smal data stored in Hadoop | Extract  | 


### Story

Suppose we are data analysts at an e-commerce company and have demographic data about our customers available to us from our sales records.  Our marketing department would like to start several regional ad campaigns to help increase business.  The marketing department has asked us to recommend where, and to whom they should focus their ad campaign.  To help us conduct our analysis, they have provided us with a large consumer dataset they recently purchased.  Presume this dataset contains related demographic information about potential customers and is stored in Hadoop.



(Logistic regression analysis)







### Approach

1. In R, build a logistic regression model from the sales records (optional: if sales records are stored in Hadoop, take a random sample of users from this)
2. Use R/Hadoop to (map) use the regression model to estimate probabilities that potential customers will buy our products, and (reduce) combine the raw data into regional summaries
3. Use R/Hadoop to retrieve this summary data, and conduct follow-on analyses in R to make recommendations to the marketing department.

### Code

```
asdfafasldkfjasldkfjas
```

### Variations (Play with it!)

- Try applying a different logistic regression or prediction/classification model
- Try extracting different summary data from reduce step 





# Conclusion

## Further reading


### R/Hadoop integrations

- Airplane dataset via RHIPE
- k-means via R/Hadoop
- biglm

### Hadoop information

- asdf