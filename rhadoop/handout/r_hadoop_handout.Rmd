

---
title: "Computing with R and Hadoop"
output: 
  html_document:
    number_sections: yes
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---


<!---

---
title: "Untitled"
output:
  ioslides_presentation: default
  beamer_presentation:
    toc: yes
---




---!>



* * * 

# Learning goals

1. Learn what Hadoop is
2. Learn about current R/Hadoop integrations
3. Learn *when* to use R with Hadoop
4. Learn *how* to use R with Hadoop (by example)

# Brief introduction to Hadoop

Hadoop is currently a widely used open source distributed computing platform (read: "cloud computing software").  Here's how the project briefly describes itself:



> [Apache Software Foundation - What is Apache Hadoop?](http://hadoop.apache.org/#What+Is+Apache+Hadoop%3F) 
>   <br />
>   <br />
> The Apache Hadoop project develops open-source software for reliable, scalable, **distributed computing**. The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering **local computation and storage**...
>  <br />
>  <br />
> (emphasis added)  


## Key features

Two of Hadoop's main features are particularly relevant to this talk:

| Feature | Solves problem |
| :- | :- |
| Distributed storage | How do we easily store and access large datasets? |
| Distributed/batch computing | How do we quickly run analyses on large datasets? |


## Overview


**Hadoop is software that...** 

<!--- need to add pictures for each of these points -->

1. Links together servers to form a (storage + computing) cluster.

2. Creates a distributed file system called **hdfs** which splits large data files into smaller pieces that servers across the cluster store.

    <!--- answer q: how does distributed storage facilitate distributed computing -->
    
3. Uses the MapReduce programming model to implement distributed (i.e., parallel) computing.

    <!--- answer q: how does mapreduce work -->
    
    *Note:* MapReduce achieves parallelization by running separate copies of an analysis  on the smaller pieces of data files stored across the cluster.  Parallelization will not occur when a data file is ``too small'' to be split into smaller pieces (by default less than 64MB or 128MB).

4. Hadoop's natural computing strength is to apply a function to **extract** "data" from each record in a dataset, **group** the function's outputs, and **compute** summaries of the grouped outputs.

    *Examples:* 
    
    1. Given a dataset of tax records for many individuals, you could easily **extract** salaries, **group** them by city, and **compute** average salaries by city with Hadoop.
    
    2. Given a large collection of Twitter tweets, with Hadoop you could easily apply natural language processing algorithms to **extract** tweet topics, **group** by date, and **compute** the most popular topics over time.
    
5. Although this is getting better, Hadoop less naturally applies iterative algorithms to datasets since iterative algorithms don't as immediately fit into the **extract**, **group**, and **compute/summarize** pattern.

    *Examples:*
    
    1. Given the same collection of tweets, it would take more programming and computational effort to compute clusters of users who most often discuss similar topics.
    
    2. Given a large dataset of sales records, it would take more programming and computational effort  to use the entire dataset to fit a logistic regression model that could help predict which customers will buy different products based on demographic information.
    
    *Note:* Sometimes data storage tricks and approximate algorithms can be used to help achieve efficient parallelization for these and related types of problems.



# R/Hadoop integrations

R/Hadoop **integrations** let people use Hadoop to execute R code and also use R code to access data stored in Hadoop.  Cloud computing and statistics are both contemporary, related topics so practitioners naturally wanted to combine the two.

## Key integration projects

| Project | Sponsors/Maintainers | 
| :- | :- | 
| RHadoop | RevolutionAnalytics |
| RHIPE | Loosely, people associated with the Deptartment of Statistics at Purdue |




## When to consider integrating R and Hadoop

You should consider using an R/Hadoop integration when the computational requirements of your project align with the natural strengths of R and Hadoop. In particular, there are several factors you should especially consider:

| Factor | Mantra | Guideline | 
| :- | :- | :- |
| R's natural strength | Use R for statistical computing | Consider integrating when your project can be solved using code available in R, or when it is not easily solved in other languages | 
| Hadoop's natural strength | Use Hadoop for distributed storage & batch computing | Consider integrating when your problem requires lots of storage or when it could benefit from parallelization  | 
| Coding effort | Work smart, not hard | R and Hadoop are tools, not ``cure-all'' panaceas. Consider **not** integrating if it is easier to solve your problem with other tools |
| Processing time | Work smart, not hard (2) | Although some problems can benefit from parallelization, consider **not** integrating if the gains are negligible since this can help you reduce the complexity of your project



The following table applies these general ideas to common analytic scenarios:


| Scenario | Use R/Hadoop? | Why? | Example
| :- | :-: | :- | :- |
| Analyzing small data stored in Hadoop | Y | R can quickly download data analyze it locally | Want to analyze summary datasets derived from map reduce jobs done in Hadoop |
| Extracting complex features from large data stored in Hadoop | Y | R has more built-in and contributed functions that analyze data than many standard programming languages | R is a natural language to use to write an algorithm or classifier that extracts information about objects contained in images |
| Applying prediction and classification models to datasets | Y | R is better at modeling than many standard programming languages | Using a logistic regression model to generate predictions in a large dataset |
| Implementing an "iteration-based" machine learning algorithm | Maybe | 1) Other languages may be faster than R for your analysis, 2) Hadoop reads and writes a lot of data to disks, other "big data" tools, like Spark (and SparkR) are designed for speed in these scenarios by working in memory | Training a k-means classification algorithm or logistic regression on a large dataset |
| Simple pre-processing of large data stored in Hadoop | N | Standard programming languages are much faster than R at executing many basic text and image processing tasks | Pre-processing twitter tweets for use in a natural language processing project |




# Example integrations

## Warm up

### Goals

1. Test connections between R and Hadoop
2. Run a basic script 


### Story


### Code 

Open "The Hadoop UI" (HUE) so you can monitor the progress of jobs you submit by visiting http://192.168.1.105:8888/jobbrowser/  The username and password are both **cloudera**.  You can also view files stored in Hadoop by visiting http://192.168.1.105:8888/filebrowser/#/


RHadoop can only be used from an R console on a machine in the Hadoop cluster.  Run the following code from your computer's command line to log in to the Hadoop node (the password is **cloudera**):

```
ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null cloudera@192.168.1.105

```


Still from your computer's command line, start R by typing ``R''.  The following code is the R script you can copy and paste to run this warm up example.

```
# Example based on RevolutionAnalytics documentation "Mapreduce in R"
# https://github.com/RevolutionAnalytics/rmr2/blob/master/docs/tutorial.md
  
library(rmr2)
library(rhdfs)

#
# "Connect" to Hadoop from R
#

# rhdfs uses applications installed at this location to read and write data 
#   from Hadoop
Sys.setenv(HADOOP_CMD='/usr/bin/hadoop')

# rmr2 uses HADOOP_STREAMING to send code to Hadoop
Sys.setenv(HADOOP_STREAMING='/usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming.jar')

  
#
# Create a dataset and store it in Hadoop
#


# create a vector of random numbers
groups = rbinom(32, n = 50, prob = 0.4)

# save the numbers to a temporary file in hdfs (Hadoop format)
groups.hdfs = to.dfs(groups)

# look at the name of the temporary file
#   Note: use HUE's file browser to go and look at this file, you will notice
#   that files, by default, are saved in a compressed binary "Hadoop" format 
groups.hdfs()

# save the same numbers to a temporary csv file in hdfs
groups.hdfs.csv = to.dfs(groups, format='csv')

# look at the name of the temporary csv file
#   Again, use HUE's file browser to go and look at this file. Read the R 
#   documentation, ?make.input.format or ?to.dfs for more details about file 
#   formats
groups.hdfs.csv()


#
# Analyze the data
#

# set up the map (i.e., "extraction") function
counting.mapper = function(., v) {
  # extract "counting element" of 1, group by value of line
  keyval(v, 1)
}

# set up the reduce (i.e., "compute/summarize") function
counting.reducer = function(k, vv) {
  # sum up the number of times the k^th value was seen in the dataset
  keyval(k, sum(vv))
}


# set up and execute the map reduce job that analyzes the data in Hadoop
#  by default, mapreduce() returns the filename where the data is stored
mapred.result = mapreduce(
  input = groups.hdfs,
  map = counting.mapper,
  reduce = counting.reducer
)

# load the results into R
analysis.results = from.dfs(mapred.result)
          

# look at the results
analysis.results
          
# compare the results with the same analysis done completely in R
#   remember: this is a trivial example
tapply(groups, groups, length)  

```




## Applying regression models, analyzing output


### Goals

Types of integration to demonstrate


| Scenario | How? |
| :- | :- |
| Applying prediction and classification models to datasets | We plan to use our sales records to build a logistic regression model that can help predict which customers in the consumer dataset are likely to buy our products |
| Analyzing smal data stored in Hadoop | Extract  | 


### Story

Suppose we are data analysts at an e-commerce company and have demographic data about our customers available to us from our sales records.  Our marketing department would like to start several regional ad campaigns to help increase business.  The marketing department has asked us to recommend where, and to whom they should focus their ad campaign.  To help us conduct our analysis, they have provided us with a large consumer dataset they recently purchased.  Presume this dataset contains related demographic information about potential customers and is stored in Hadoop.



(Logistic regression analysis)







### Approach

1. In R, build a logistic regression model from the sales records (optional: if sales records are stored in Hadoop, take a random sample of users from this)
2. Use R/Hadoop to (map) use the regression model to estimate probabilities that potential customers will buy our products, and (reduce) combine the raw data into regional summaries
3. Use R/Hadoop to retrieve this summary data, and conduct follow-on analyses in R to make recommendations to the marketing department.

### Code

```
asdfafasldkfjasldkfjas
```

### Variations (Play with it!)

- Try applying a different logistic regression or prediction/classification model
- Try extracting different summary data from reduce step 





# Conclusion

## Further reading

https://github.com/RevolutionAnalytics/rmr2/blob/master/docs/tutorial.md


### R/Hadoop integrations

- Airplane dataset via RHIPE
- k-means via R/Hadoop
- biglm

### Hadoop information

- asdf