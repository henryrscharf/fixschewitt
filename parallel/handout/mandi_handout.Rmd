---
title: "parallel"
output:
  html_document:
    highlight: tango
    keep_md: yes
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---
<!--
% CSP course: using the parallel package
% Miranda Fix
% 12/11/14
-->


```{r setup, include=FALSE}
# set global chunk options
library(knitr)
opts_chunk$set(cache=FALSE)
```


# Why parallel?
We've already seen the `doParallel` package, which acts as an interface between `foreach` and `parallel`

The `parallel` package is essentially a merger of `multicore` and `snow`

- Provides drop-in replacements for most of the functionality of those packages, with integrated handling of random-number generation
- By default, `doParallel` uses `multicore` functionality on Unix-like systems and `snow` functionality on Windows (you can also use the `snow` functionality to execute on a cluster of computers for both systems)

Whereas `foreach` was a parallel analogue of a `for` loop, the `parallel` package provides handy analogues of `apply` functions

# Getting started

## Parallel backend
To get started, we must first make the desired number of cores available to R by registering a parallel backend. We can do this with the `doParallel` package.

```{r clustersetup, message=FALSE}
library(doParallel)

# Determine number of CPU cores in your machine
nCores = detectCores()

# Create cluster with desired number of cores
cl = makeCluster(nCores)

# Register cluster
registerDoParallel(cl)
```

## Parallel apply
We've seen how we can move from a traditional `for` loop to `foreach`. Similarly, we can speed up traditional `apply` functions by using their parallel analogues. Let's start with a simple example (for now ignoring potential issues with random number generation).

```{r apply}

N = 100
input = seq_len(N) # same as 1:N but more robust
input.list = as.list(input)

testFun = function(i){
  mu = mean(runif(1e+06, 0, i))
  return(mu)
}

system.time(sapply(input, testFun))
system.time(parSapply(cl, input, testFun))

system.time(lapply(input.list, testFun))
system.time(parLapply(cl, input.list, testFun))
system.time(mclapply(input.list, testFun, mc.cores=nCores)) # not available on Windows

```

## pvec
We can also parallelize a vector map function using `pvec` (not available on Windows). This splits the vector and submits each part to one core, so it only works for functions such that `c(FUN(x[1]), FUN(x[2]))` is equivalent to `FUN(x[1:2])`. It is often only worthwhile on very long vectors and for computationally intensive calculations, such as this example of evaluating the Matern covariance function:

```{r pvec, message=FALSE}
library(fields)

d = runif(5e+06, 0.1, 10)

system.time(Matern(d))
system.time(pvec(d, Matern, mc.cores=nCores))
```

# Random number generation
Once again, we need to be careful when parallelizing a process which generates (psuedo-)random numbers. We want the different processes to run independent (and preferably reproducible) random-number streams. We can do this using `clusterSetRNGStream`. Let's go back and fix our first example:

```{r rngstream}
N = 100
input = seq_len(N)
testFun = function(i){
  mu = mean(runif(1e+06, 0, i))
  return(mu)
}

# RNGkind("L-Ecuyer-CMRG") # Automatically uses L'Ecuyer's algorithm

clusterSetRNGStream(cl, iseed=0)
res1 = parSapply(cl, input, testFun)

clusterSetRNGStream(cl, iseed=0)
res2 = parSapply(cl, input, testFun)

identical(res1, res2)
```

# Bootstrapping

Bootstrapping is another example of an embarassingly parallel task. In the example below, we use bootstrapping to generate a 95% confidence interval for R-squared of the linear regression of miles per gallon on car weight and displacement. Using `parLapply` we can split the n*1000 bootstrap replicates between the n cores.

```{r boot}
library(boot)

run1 = function(...){
  library(boot)
  rsq = function(formula, data, indices){
    d = data[indices,]
    fit = lm(formula, data=d)
    return(summary(fit)$r.square)
  }
  boot(data=mtcars, statistic=rsq, R=1000, formula=mpg~wt+disp)
}

system.time(car.boot <- do.call(c, lapply(seq_len(nCores), run1)))

clusterSetRNGStream(cl, iseed=123)
system.time(car.boot2 <- do.call(c, parLapply(cl, seq_len(nCores), run1)))
boot.ci(car.boot2, type="bca")

```

Note that the `boot` package has built-in parallel support, so we can also simply run the following:

```{r boot2}
rsq = function(formula, data, indices){
  d = data[indices,]
  fit = lm(formula, data=d)
  return(summary(fit)$r.square)
}

nBoot = nCores*1000
set.seed(123, kind="L'Ecuyer")
system.time(
  car.boot3 <- boot(data=mtcars, statistic=rsq, R=nBoot, formula=mpg~wt+disp, 
                    parallel="multicore", ncpus=4)
)
boot.ci(car.boot3, type="bca")
```

# Stop cluster
It's a good idea to stop your cluster when you are done using it.

```{r}
stopCluster(cl)
```

