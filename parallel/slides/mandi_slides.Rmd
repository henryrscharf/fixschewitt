---
title: "parallel"
author: "Miranda Fix"
date: "December 11, 2014"
output:
  ioslides_presentation:
    highlight: tango
    incremental: yes
    transition: faster
---

```{r setup, include=FALSE}
# set global chunk options
library(knitr)
opts_chunk$set(cache=FALSE)
opts_chunk$set(eval=TRUE)
```

# Why parallel?

## Why parallel?
We've already seen the `doParallel` package, which acts as an interface between `foreach` and `parallel`. Whereas `foreach` was a parallel analogue of a `for` loop, the `parallel` package provides handy analogues of `apply` functions

The `parallel` package merges `multicore` and `snow`

- Provides drop-in replacements for most of their functionality, with integrated handling of random-number generation
- By default, uses `multicore` functionality on Unix-like systems and `snow` functionality on Windows (we can also use the `snow` functionality to execute on a cluster for both systems)

# Getting started

## Parallel backend
To get started, we must first make the desired number of cores available to R by registering a parallel backend. We can do this with the `doParallel` package.

```{r clustersetup, message=FALSE}
library(doParallel)

# Determine number of CPU cores in your machine
nCores = detectCores()

# Create cluster with desired number of cores
cl = makeCluster(nCores)

# Register cluster
registerDoParallel(cl)

```

## Parallel apply
We've seen how we can move from a traditional `for` loop to `foreach`. Similarly, we can speed up traditional `apply` functions by using their parallel analogues.

```{r apply}

N = 100
input = seq_len(N) # same as 1:N but more robust
input.list = as.list(input)

testFun = function(i){
  mu = mean(runif(1e+06, 0, i))
  return(mu)
}
```

----

```{r apply2.1}
system.time(sapply(input, testFun))
system.time(parSapply(cl, input, testFun))
```
----
```{r apply2.2}
system.time(lapply(input.list, testFun))
system.time(parLapply(cl, input.list, testFun))
system.time(mclapply(input.list, testFun, mc.cores=nCores))  
# not available on Windows
```


## pvec
We can also parallelize a vector map function using `pvec`. This splits the vector and submits each part to one core. It is often only worthwhile on very long vectors and for computationally intensive calculations.

```{r pvec, message=FALSE}
library(fields)
d = runif(5e+06, 0.1, 10)
system.time(Matern(d))
system.time(pvec(d, Matern, mc.cores=nCores))
```

# Random number generation

----

Once again, we need to be careful when parallelizing a process which generates (psuedo-)random numbers.

We want the different processes to run independent (and preferably reproducible) random-number streams. We can do this using `clusterSetRNGStream`.

Let's go back and fix our first example:

```{r rngstream}
N = 100
input = seq_len(N)

testFun = function(i){
  mu = mean(runif(1e+06, 0, i))
  return(mu)
}
```

----

```{r rngstream2}
# RNGkind("L-Ecuyer-CMRG") # Automatically uses L'Ecuyer's algorithm

clusterSetRNGStream(cl, iseed=0)
res1 = parSapply(cl, input, testFun)

clusterSetRNGStream(cl, iseed=0)
res2 = parSapply(cl, input, testFun)

identical(res1, res2)
```

# Bootstrapping

----

**Bootstrapping** is another example of an embarassingly parallel task. In the example below, we use bootstrapping to generate a 95% confidence interval for R-squared of a linear regression. Using `parLapply` we can split the n*1000 bootstrap replicates between the n cores.

```{r boot, message=FALSE}
library(boot)

run1 = function(...){
  library(boot)
  rsq = function(formula, data, indices){
    d = data[indices,]
    fit = lm(formula, data=d)
    return(summary(fit)$r.square)
  }
  boot(data=mtcars, statistic=rsq, R=1000, formula=mpg~wt+disp)
}
```

----

```{r boot2}
system.time(car.boot <- do.call(c, lapply(seq_len(nCores), run1)))

clusterSetRNGStream(cl, iseed=123)
system.time(
  car.boot2 <- do.call(c, parLapply(cl, seq_len(nCores), run1))
)
```

----

```{r bootCI}
boot.ci(car.boot2, type="bca")

```

----

Note that the `boot` package has built-in parallel support, so we can also simply run the following:

```{r boot3}
rsq = function(formula, data, indices){
  d = data[indices,]
  fit = lm(formula, data=d)
  return(summary(fit)$r.square)
}
nBoot = nCores*1000
set.seed(123, kind="L'Ecuyer")
system.time(
  car.boot3 <- boot(data=mtcars, statistic=rsq, R=nBoot, 
              formula=mpg~wt+disp, parallel="multicore", ncpus=4)
)
```

## Stop cluster
It's a good idea to stop your cluster when you are done using it.

```{r}
stopCluster(cl)
```

## <span class = "nine">references</span>

### Other tutorials
[Package ‘parallel’](http://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf)